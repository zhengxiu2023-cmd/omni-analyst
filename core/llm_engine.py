# -*- coding: utf-8 -*-
"""
ğŸ§  core/llm_engine.py â€” æœ¬åœ°å¤§æ¨¡å‹æçº¯å¼•æ“ (Local LLM Engine)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
èŒè´£ï¼š
  - å°è£…å¯¹æœ¬åœ° Ollama API çš„è°ƒç”¨ï¼Œå±è”½åº•å±‚ç»†èŠ‚ã€‚
  - å°† DATA_CONTRACTS ä¸­å®šä¹‰çš„ System Prompt æ³¨å…¥æ¯æ¬¡è¯·æ±‚ï¼Œ
    ç¡®ä¿å¤§æ¨¡å‹å§‹ç»ˆåœ¨"è¶…æ™¯æ°”ä»·å€¼æŠ•æœº"çš„åˆ¤åˆ†æ¡†æ¶ä¸‹è¿ä½œã€‚
  - è§£æ Ollama è¿”å›çš„ JSON æ ¼å¼è¯„åˆ†ç»“æœï¼Œæ˜ å°„ä¸º NewsItem çš„ score å­—æ®µã€‚
  - å…¨é¢çš„å…œåº•æœºåˆ¶ï¼šä»»ä½•å¼‚å¸¸å‡é™é»˜æ•è·ï¼Œè¿”å›å®‰å…¨é»˜è®¤å€¼ï¼Œç»ä¸é˜»å¡ä¸»çº¿ç¨‹ã€‚

é…ç½®æ¥æºï¼šconfig.py > LLM_CONFIGï¼ˆOLLAMA_API / MODEL_NAME / SYSTEM_PROMPT / TIMEOUTï¼‰
"""

import json
import logging
import re
from dataclasses import dataclass
from typing import Optional

import requests
import requests.exceptions

from config import LLM_CONFIG

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# å†…éƒ¨æ•°æ®ç»“æ„ï¼šLLM å•æ¬¡è¯„åˆ†ç»“æœ
# ---------------------------------------------------------------------------
@dataclass
class LLMScore:
    """
    Ollama å•æ¬¡æƒ…æŠ¥è¯„åˆ†çš„ç»“æ„åŒ–ç»“æœã€‚

    Attributes:
        score:     å¥‡ç‚¹å…±æŒ¯è¯„åˆ†ï¼ˆ0 åƒåœ¾/1 æ™®é€š/2 æ ¸å¿ƒæ‹ç‚¹ï¼‰ï¼Œå¯¹åº” NewsItem.scoreã€‚
        reasoning: å¤§æ¨¡å‹ç»™å‡ºçš„æç®€åˆ¤åˆ†ç†ç”±ï¼Œå¯¹åº” NewsItem.llm_reasoningã€‚
        success:   æœ¬æ¬¡ LLM è°ƒç”¨æ˜¯å¦æˆåŠŸè§£æï¼ˆç”¨äºä¸Šå±‚åˆ¤æ–­æ˜¯å¦é™çº§åˆ°æ­£åˆ™ï¼‰ã€‚
    """
    score: int = 0
    reasoning: str = "LLMå¼•æ“è§£æå¤±è´¥æˆ–è¶…æ—¶"
    success: bool = False


# ---------------------------------------------------------------------------
# æ¨¡å—å…¬å…±æ¥å£
# ---------------------------------------------------------------------------
def evaluate_intel(text: str, title: Optional[str] = None) -> LLMScore:
    """
    è°ƒç”¨æœ¬åœ° Ollama å¯¹ä¸€æ¡æƒ…æŠ¥è¿›è¡Œè¶…æ™¯æ°”è¯„åˆ†ã€‚

    Args:
        text:  æƒ…æŠ¥çš„æ­£æ–‡å†…å®¹ï¼ˆå¦‚æ–°é—»å…¨æ–‡ã€å…¬å‘Šæ‘˜è¦ï¼‰ã€‚
        title: æƒ…æŠ¥æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰ï¼Œä¼šå’Œ text æ‹¼æ¥åä¸€èµ·é€å…¥æ¨¡å‹ï¼Œæå‡ä¸Šä¸‹æ–‡ç²¾åº¦ã€‚

    Returns:
        LLMScoreï¼šåŒ…å« score (0/1/2)ã€reasoning å’Œ success æ ‡å¿—ã€‚
        è‹¥ LLM ä¸å¯ç”¨æˆ–è§£æå¤±è´¥ï¼Œè¿”å› score=0 çš„å®‰å…¨é»˜è®¤å€¼ã€‚
    """
    # å¦‚æœ LLM åŠŸèƒ½è¢«é…ç½®ä¸ºå…³é—­ï¼Œç›´æ¥è¿”å›é»˜è®¤å€¼ï¼ˆé™çº§åˆ°æ­£åˆ™æ¨¡å¼ï¼‰
    if not LLM_CONFIG.get("ENABLE", True):
        logger.debug("[LLMå¼•æ“] åŠŸèƒ½å·²åœ¨é…ç½®ä¸­ç¦ç”¨ï¼Œè·³è¿‡è°ƒç”¨ã€‚")
        return LLMScore(reasoning="LLMåŠŸèƒ½å·²å…³é—­ï¼Œä½¿ç”¨æ­£åˆ™é™çº§æ¨¡å¼")

    # æ‹¼æ¥ç”¨æˆ·è¾“å…¥ï¼šæ ‡é¢˜æƒé‡é«˜ï¼Œæ”¾åœ¨æœ€å‰
    user_prompt: str = f"æ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{text}" if title else f"å†…å®¹ï¼š{text}"

    # ç»„è£… Ollama API Payload
    # format="json" å¼ºåˆ¶ Ollama è¾“å‡ºåˆæ³• JSONï¼Œé¿å… markdown ä»£ç å—å¹²æ‰°è§£æ
    payload: dict = {
        "model": LLM_CONFIG["MODEL_NAME"],
        "system": LLM_CONFIG["SYSTEM_PROMPT"],
        "prompt": user_prompt,
        "stream": False,        # å•æ¬¡è¯·æ±‚æ¨¡å¼ï¼Œä¸ä½¿ç”¨æµå¼è¿”å›
        "format": "json",       # å¼ºåˆ¶ JSON æ ¼å¼è¾“å‡ºï¼ˆOllama >= 0.1.23 æ”¯æŒï¼‰
    }

    try:
        response = requests.post(
            LLM_CONFIG["OLLAMA_API"],
            json=payload,
            timeout=LLM_CONFIG["TIMEOUT"],
        )
        response.raise_for_status()

        # è§£æ Ollama çš„å“åº”ä½“ï¼Œå–å‡º "response" å­—æ®µï¼ˆå³æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼‰
        raw_text: str = response.json().get("response", "")
        return _parse_llm_response(raw_text)

    except requests.exceptions.Timeout:
        # è¶…æ—¶æ˜¯æœ€å¸¸è§çš„å¤±è´¥åœºæ™¯ï¼Œå•ç‹¬è®°å½•ä»¥ä¾¿ç›‘æ§
        logger.warning("[LLMå¼•æ“] è¯·æ±‚è¶…æ—¶ (%.1fs)ï¼Œå·²é™çº§å¤„ç†ã€‚", LLM_CONFIG["TIMEOUT"])
        return LLMScore(reasoning="LLMå¼•æ“è¶…æ—¶ï¼Œå·²é™çº§åˆ°æ­£åˆ™æ¨¡å¼")

    except requests.exceptions.ConnectionError:
        # Ollama æœåŠ¡æœªå¯åŠ¨æˆ–ç«¯å£æœªå¼€æ”¾
        logger.warning("[LLMå¼•æ“] æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ (%s)ï¼Œè¯·ç¡®è®¤æœåŠ¡å·²å¯åŠ¨ã€‚", LLM_CONFIG["OLLAMA_API"])
        return LLMScore(reasoning="OllamaæœåŠ¡æœªå¯åŠ¨æˆ–è¿æ¥è¢«æ‹’")

    except requests.exceptions.RequestException as exc:
        logger.error("[LLMå¼•æ“] ç½‘ç»œè¯·æ±‚å¼‚å¸¸: %s", exc)
        return LLMScore(reasoning=f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {type(exc).__name__}")

    except Exception as exc:  # å…œåº•ï¼šæ•è·ä¸€åˆ‡æœªé¢„æ–™çš„å¼‚å¸¸
        logger.error("[LLMå¼•æ“] æœªçŸ¥å¼‚å¸¸: %s", exc)
        return LLMScore(reasoning=f"æœªçŸ¥å¼‚å¸¸: {type(exc).__name__}")


def check_availability() -> bool:
    """
    å¿«é€Ÿæ£€æŸ¥æœ¬åœ° Ollama æœåŠ¡æ˜¯å¦åœ¨çº¿ï¼ˆç”¨äºå¯åŠ¨æ—¶çš„å¥åº·æ£€æµ‹ï¼‰ã€‚

    Returns:
        True è¡¨ç¤º Ollama æœåŠ¡å¯ç”¨ï¼ŒFalse è¡¨ç¤ºä¸å¯ç”¨ï¼ˆéœ€é™çº§åˆ°æ­£åˆ™æ¨¡å¼ï¼‰ã€‚
    """
    from config import API_CONFIG
    try:
        resp = requests.get(
            API_CONFIG["OLLAMA_HEALTH"],
            timeout=API_CONFIG["LLM_HEALTH_TIMEOUT"],
        )
        return resp.status_code == 200
    except requests.exceptions.RequestException:
        return False


# ---------------------------------------------------------------------------
# ç§æœ‰è¾…åŠ©å‡½æ•°
# ---------------------------------------------------------------------------
def _parse_llm_response(raw_text: str) -> LLMScore:
    """
    è§£æ Ollama è¿”å›çš„ JSON å­—ç¬¦ä¸²ï¼Œæå– score å’Œ reasoningã€‚

    æœŸæœ›æ ¼å¼ï¼ˆæ¥è‡ª System Prompt çº¦å®šï¼‰ï¼š
        {"score": 2, "reasoning": "å‘ç°Type2ç°è´§æ–­ä¾›ä¿¡å·ï¼Œæåˆ°ä¸Šæ¸¸å…¨é¢å°ç›˜"}

    å®¹é”™å¤„ç†ï¼š
        - è‹¥å­—æ®µç¼ºå¤±ï¼Œscore é»˜è®¤ä¸º 0ï¼Œreasoning ä¿ç•™åŸå§‹æ–‡æœ¬ä¾¿äºæ’æŸ¥ã€‚
        - score è¶…å‡º [0, 2] èŒƒå›´æ—¶ï¼Œå¼ºåˆ¶æˆªæ–­åˆ°åˆæ³•åŒºé—´ã€‚

    Args:
        raw_text: Ollama è¿”å›çš„åŸå§‹æ–‡æœ¬ï¼ˆåº”ä¸º JSON å­—ç¬¦ä¸²ï¼‰ã€‚

    Returns:
        è§£æåçš„ LLMScore å¯¹è±¡ã€‚
    """
    if not raw_text.strip():
        logger.debug("[LLMè§£æ] æ¨¡å‹è¿”å›ç©ºæ–‡æœ¬ã€‚")
        return LLMScore(reasoning="æ¨¡å‹è¿”å›å†…å®¹ä¸ºç©º")

    try:
        # æŠ—å¹»è§‰é˜²å¾¡ï¼šå¦‚æœå°æ¨¡å‹ä¾ç„¶è¾“å‡ºäº† ```json ... ``` å—ï¼Œæ­£åˆ™å‰¥ç¦»æ‰å®ƒ
        clean_text = raw_text.strip()
        match = re.search(r"```(?:json)?\s*(.*?)\s*```", clean_text, re.DOTALL)
        if match:
            clean_text = match.group(1).strip()
            
        data: dict = json.loads(clean_text)

        # æå– scoreï¼Œå¹¶å¼ºåˆ¶çº¦æŸåœ¨ [0, 2] åŒºé—´
        raw_score = data.get("score", 0)
        score: int = max(0, min(2, int(raw_score)))

        reasoning: str = str(data.get("reasoning", "")).strip() or "æ¨¡å‹æœªç»™å‡ºç†ç”±"

        logger.debug("[LLMè§£æ] score=%d | reasoning=%s", score, reasoning)
        return LLMScore(score=score, reasoning=reasoning, success=True)

    except (json.JSONDecodeError, ValueError, TypeError) as exc:
        # JSON è§£æå¤±è´¥æ—¶ä¿ç•™åŸå§‹æ–‡æœ¬ï¼Œä¾¿äºè°ƒè¯• Prompt æ˜¯å¦éœ€è¦è°ƒæ•´
        logger.warning("[LLMè§£æ] JSONè§£æå¤±è´¥: %s | åŸå§‹è¿”å›: %.100s", exc, raw_text)
        return LLMScore(reasoning=f"JSONè§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡º: {raw_text[:80]}")
